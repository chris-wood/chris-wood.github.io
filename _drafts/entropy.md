---
layout: post
title: How Random are URIs?
---

The web is an interesting place. It's full of systems, devices, applications, resources, 
and other miscellaneaous entities that play some part in its operation or use it on
a regular basis. URIs are the identifiers or names we assign to these entities to give 
them meaning and enable interoperability. Sometimes these names are fairly deterministic
and predictable, e.g., the URI one might request to get my latest public events on Github
would be:

~~~
https://api.github.com/users/chris-wood/events
~~~

Other times, URIs contain seemingly randomly or unpredictable information such as a unique
nonce generated by a client application, encrypted value required by the server, or just
some binary data. Without peering into how URIs are generated or used, can we assess the
amount of randomness (or lack thereof) contained in these URIs? This is the question we set
out to answer in an ongoing research problem. In this post, I'll describe how we solved this
particular problem.

# Entropy Formulations

To assess the randomness of a set we turned to the traditional Shannon formulation
of entropy defined as follows:

$$
\begin{align}
H(X) = -\sum_{x \in X}P(X = x)\log(P(X = x)).
\end{align}
$$

The chain rule is useful to quantify the amount of entropy that is lost when
new information is presented. Formally, the entropy of $X$, if conditioned
on the entropy of $Y$, can decrease by at most the entropy in $Y$, i.e.,
$$
\begin{align}
H(X|Y) = H(X,Y) - H(Y),
\end{align}
$$

where $H(X,Y)$ is the joint entropy of $X$ and $Y$ defined as

$$
\begin{align}
H(X,Y) = -\sum_{x \in X}\sum_{y \in Y}\Pr(X = x, Y = y)\log(\Pr(X = x, Y = y)).
\end{align}
$$

This formulation can be generalized to support computing the conditional
and joint entropy of an arbitrary number of random variables.

# Posing URI Randomness

Consider the following simplified URI: 

~~~
www.github.com/chris-wood/
~~~

Let's rewrite this as follows:

~~~
/com/google/chris-wood
~~~

Now let me pose the following question. What if we considered this URI as an array of 
individual components (i.e., one separated by the '/' character)? Given a list of N 
URIs of length M, we can treat this as a matrix of N rows and M columns. Each entry
of which is a component of a URI. 

Let's now ask the following questions:

1. What is the amount of randomness in each component of these URIs? 
2. How much information about the i-th component is leaked by the previous
(i - 1) components?

If we treat each column of the URI component as a set of samples we can then
compute the entropy of each component. Using this, we can compute the conditional
entropy of the i-th component given the previous (i - 1) components. In the next
section, I will describe how I solved this problem.

# Computing Entropy

The last step we need to complete in our query for URI randomness is
to actually compute the entropy of a set given a list of samples. Given
the definition presented earlier, this is a fairly straightforward task. 
We just need to do the following:

1. Compute the PMF of the set by counting the number of occurrences of each
element and then dividing it by the total number of elements in the set.
2. Use the PMF to compute the entropy by looping over every element in the set.

Simple enough. The code to do this is below.

{% gist 82c5f788d697fa92c308 %}

Although correct, this code runs pretty slow a large input. Its exact 
complexity is TODO. But without a different algorithm, we can't do much 
better. So let's try to parallelize what we can. This is nothign more
than a simple map-reduce problem in disguise. We can compute the PMF
of the set by splitting diviing the input elements to multiple workers,
generating a partial count map, and then reducing the results by combining
the dictionaries. We can do the same thing when using this PMF to compute
the final entropy. Specifically, for P partitions, we compute the following:

$$
\begin{align}
\sum_{ji0}^P \sum_{x \in X_i} P(X = x)\log(P(X = x))
\end{align}
$$

The following code snippet puts these two pieces together. 

{% gist 82c5f788d697fa92c308 %}

# Performance Comparison

Did we actually do any better by computing the entropy in parallel? Let's run
some tests to find out. I downloaded the set of Cisco-generated URIs from
[icn-names.net](icn-names.net) and took a large enough sample to make for a 
visible comparison. I then parsed list of URIs to create the matrix of components. 
Finally, I ran this matrix through the entropy calculation for *each* column 
(component). The results are shown in the table below.

| Component | Sequential Entropy | Parallel Entropy |
| X | Y | Z |

TODO: do some analysis

# Randomness Results

TODO

